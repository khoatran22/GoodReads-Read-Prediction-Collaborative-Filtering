{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import statistics\n",
    "def readCSV(path):\n",
    "  f = gzip.open(path, 'rt')\n",
    "  f.readline()\n",
    "  for l in f:\n",
    "    yield l.strip().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "userIDs = {}\n",
    "itemIDs = {}\n",
    "trainSet = []\n",
    "validationSet = []\n",
    "testSet = []\n",
    "interactions = []\n",
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "iterations = 0 \n",
    "for user,book,rating in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    interactions.append((user,book,rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(interactions)\n",
    "trainSet = interactions[0:180000]\n",
    "validation = interactions[180000:190000]\n",
    "testSet = interactions[190000:200000]\n",
    "#Count number of interactions for each book\n",
    "usersPerItem = defaultdict(set) # U_i from class slides\n",
    "itemsPerUser = defaultdict(set) # I_u from class slides\n",
    "for user,book,_ in trainSet:\n",
    "    totalRead += 1\n",
    "    bookCount[book] += 1\n",
    "    usersPerItem[book].add(user)\n",
    "    itemsPerUser[user].add(book)\n",
    "items = list(usersPerItem.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create negative samples for validation set.\n",
    "validationSet = [] #(user_id, book_id, bought/ not bought)\n",
    "for user,book,d in validation:\n",
    "    validationSet.append((user,book,1))\n",
    "    bookNeg = random.choice(items) # negative sample\n",
    "    while bookNeg in itemsPerUser[user]:\n",
    "        bookNeg = random.choice(items)\n",
    "    validationSet.append((user,bookNeg,0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerBook = defaultdict(set)\n",
    "booksPerUser = defaultdict(set)\n",
    "for d in trainSet:\n",
    "    user, book = d[0], d[1]\n",
    "    usersPerBook[book].add(user)\n",
    "    booksPerUser[user].add(book)\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "def mostSimilarFast(i, j ):\n",
    "    similarities = []\n",
    "    books = booksPerUser[i] #the games that the user i played in the train set\n",
    "    users = usersPerBook[j] #the game j that users played in train set\n",
    "    for b in books:\n",
    "        similarities.append(Jaccard(users, usersPerBook[b]))\n",
    "    return similarities\n",
    "\n",
    "def cosine(s1, s2):\n",
    "    product = len(s1.intersection(s2))\n",
    "    denom = math.sqrt(len(s1)) * math.sqrt(len(s2))\n",
    "    return product/denom\n",
    "\n",
    "def mostSimilarFast2(i, j ):\n",
    "    similarities = []\n",
    "    books = booksPerUser[i] #the games that the user i played in the train set\n",
    "    users = usersPerBook[j] #the game j that users played in train set\n",
    "    for b in books:\n",
    "        similarities.append(cosine(users, usersPerBook[b]))\n",
    "    return similarities\n",
    "\n",
    "def euclidean_distance(s1, s2):\n",
    "    count = 0\n",
    "    for i in s1:\n",
    "        if i not in s2:\n",
    "            count = count + 1\n",
    "    for i in s2:\n",
    "        if i not in s1:\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "def mostSimilarFast3(i, j ):\n",
    "    similarities = []\n",
    "    books = booksPerUser[i] #the games that the user i played in the train set\n",
    "    users = usersPerBook[j] #the game j that users played in train set\n",
    "    for b in books:\n",
    "        similarities.append(euclidean_distance(users, usersPerBook[b]))\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new file for is_game_valSet\n",
    "test = open(\"validation.txt\", 'w')\n",
    "test.write(\"userID-bookID,prediction\\n\")\n",
    "for d in validationSet:\n",
    "    test.write(d[0] + '-' + d[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#popularity for each book\n",
    "bookCount = defaultdict(int)\n",
    "totalbought = 0\n",
    "\n",
    "for user,book,_ in trainSet:\n",
    "    bookCount[book] += 1\n",
    "    totalbought += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "returnPopular = set()\n",
    "returnPopularCnt = set()\n",
    "count = 0\n",
    "#Extract books that account for 40% of all interactions\n",
    "for ic, i in mostPopular:\n",
    "  count += ic\n",
    "  returnPopular.add(i)\n",
    "  returnPopularCnt.add((ic,i))\n",
    "  if count > totalRead*0.65: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline popularity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lst = []\n",
    "count = 0\n",
    "labels = []\n",
    "for sample in validationSet:\n",
    "    user = sample[0]\n",
    "    book = sample[1]\n",
    "    if book in returnPopular:\n",
    "        predictions_lst.append(1)\n",
    "    else:\n",
    "        predictions_lst.append(0)\n",
    "    labels.append(sample[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6538"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy\n",
    "count = 0\n",
    "for i in range(len(predictions_lst)):\n",
    "    if predictions_lst[i] == validationSet[i][2]:\n",
    "        count += 1\n",
    "accuracy = count/len(predictions_lst)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#popularity       \n",
    "popular = {}\n",
    "\n",
    "\n",
    "predictions = open(\"predictions_bought.txt\", 'w')\n",
    "for l in open(\"validation.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    \n",
    "    for num, book in returnPopularCnt:  \n",
    "        if num > 10 and b == book:\n",
    "            popular[b] = 1\n",
    "            break\n",
    "        popular[b] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard similarity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lst = []\n",
    "count = 0\n",
    "predictions = open(\"predictions_bought.txt\", 'w')\n",
    "for l in open(\"validation.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    \n",
    "    sim1 = mostSimilarFast(u, b)\n",
    "\n",
    "    if len(sim1) == 0:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "        predictions_lst.append(0)\n",
    "        count +=1\n",
    "    elif statistics.mean(sim1) > 0.002 or popular[b] == 1:\n",
    "        predictions.write(u + '-' + b + \",1\\n\")\n",
    "        predictions_lst.append(1)\n",
    "        count +=1\n",
    "    elif statistics.mean(sim1) <= 0.002 or popular[b] == 0:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "        predictions_lst.append(0)\n",
    "        count+=1\n",
    "    else:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "        predictions_lst.append(0)\n",
    "        count+=1\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.683735392948493"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy\n",
    "count = 0\n",
    "for i in range(len(predictions_lst)):\n",
    "    if predictions_lst[i] == validationSet[i][2]:\n",
    "        count += 1\n",
    "accuracy = count/len(predictions_lst)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lst = []\n",
    "count = 0\n",
    "predictions = open(\"predictions_bought.txt\", 'w')\n",
    "for l in open(\"validation.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    \n",
    "    sim2 = mostSimilarFast2(u, b)\n",
    "\n",
    "    if len(sim2) == 0:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "        predictions_lst.append(0)\n",
    "        count +=1\n",
    "    elif statistics.mean(sim2) > 0.004 or popular[b] == 1:\n",
    "        predictions.write(u + '-' + b + \",1\\n\")\n",
    "        predictions_lst.append(1)\n",
    "        count +=1\n",
    "    elif statistics.mean(sim2) <= 0.004 or popular[b] == 0:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "        predictions_lst.append(0)\n",
    "        count+=1\n",
    "    else:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "        predictions_lst.append(0)\n",
    "        count+=1\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6771152013641607"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy\n",
    "count = 0\n",
    "for i in range(len(predictions_lst)):\n",
    "    if predictions_lst[i] == validationSet[i][2]:\n",
    "        count += 1\n",
    "accuracy = count/len(predictions_lst)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean distance similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lst = []\n",
    "count = 0\n",
    "predictions = open(\"predictions_bought.txt\", 'w')\n",
    "for l in open(\"validation.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    \n",
    "    sim3 = mostSimilarFast3(u, b)\n",
    "\n",
    "    if len(sim3) == 0:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "        predictions_lst.append(0)\n",
    "        count +=1\n",
    "    elif statistics.mean(sim3) > 93:\n",
    "        predictions.write(u + '-' + b + \",1\\n\")\n",
    "        predictions_lst.append(1)\n",
    "        count +=1\n",
    "    elif statistics.mean(sim3) <= 93:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "        predictions_lst.append(0)\n",
    "        count+=1\n",
    "    else:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "        predictions_lst.append(0)\n",
    "        count+=1\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6145242991122924"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy\n",
    "count = 0\n",
    "for i in range(len(predictions_lst)):\n",
    "    if predictions_lst[i] == validationSet[i][2]:\n",
    "        count += 1\n",
    "accuracy = count/len(predictions_lst)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
